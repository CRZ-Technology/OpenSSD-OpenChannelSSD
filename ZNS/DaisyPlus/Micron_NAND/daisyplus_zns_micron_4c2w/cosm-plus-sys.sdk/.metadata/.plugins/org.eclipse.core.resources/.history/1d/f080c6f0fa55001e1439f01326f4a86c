//////////////////////////////////////////////////////////////////////////////////
// nvme_io_cmd.c for Cosmos+ OpenSSD
// Copyright (c) 2016 Hanyang University ENC Lab.
// Contributed by Yong Ho Song <yhsong@enc.hanyang.ac.kr>
//				  Youngjin Jo <yjjo@enc.hanyang.ac.kr>
//				  Sangjin Lee <sjlee@enc.hanyang.ac.kr>
//				  Jaewook Kwak <jwkwak@enc.hanyang.ac.kr>
//
// This file is part of Cosmos+ OpenSSD.
//
// Cosmos+ OpenSSD is free software; you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation; either version 3, or (at your option)
// any later version.
//
// Cosmos+ OpenSSD is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
// See the GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with Cosmos+ OpenSSD; see the file COPYING.
// If not, see <http://www.gnu.org/licenses/>.
//////////////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////////////
// Company: ENC Lab. <http://enc.hanyang.ac.kr>
// Engineer: Sangjin Lee <sjlee@enc.hanyang.ac.kr>
//			 Jaewook Kwak <jwkwak@enc.hanyang.ac.kr>
//
// Project Name: Cosmos+ OpenSSD
// Design Name: Cosmos+ Firmware
// Module Name: NVMe IO Command Handler
// File Name: nvme_io_cmd.c
//
// Version: v1.0.1
//
// Description:
//   - handles NVMe IO command
//////////////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////////////
// Revision History:
//
// * v1.0.1
//   - header file for buffer is changed from "ia_lru_buffer.h" to "lru_buffer.h"
//
// * v1.0.0
//   - First draft
//////////////////////////////////////////////////////////////////////////////////


#include "xil_printf.h"
#include "debug.h"
#include "io_access.h"

#include "nvme.h"
#include "host_lld.h"
#include "nvme_io_cmd.h"

#include "../ftl_config.h"
#include "../request_transform.h"

#include "zns.h"

void handle_nvme_io_erase(unsinged int cmdSlotTag, uint64_t zslba, uint64_t nlb)
{
    NVME_COMPLETION nvmeCPL;
    unsigned int blockNo, dieNo, reqSlotTag, startBlockNo, endBlockNo;
 
    if(!nbl)
    {
        set_auto_nvme_cpl(cmdSlotTag, 0, 0);
        return;
    }

    startBlockNo = zslba / NVME_BLOCKS_PER_SLICE / USER_PAGES_PER_BLOCK / USER_CHANNELS / USER_WAYS;
    endBlockNo = startBlockNo + nlb / NVME_BLOCKS_PER_SLICE / USER_PAGES_PER_BLOCK / USER_CHANNELS / USER_WAYS;
    if(nlb % (NVME_BLOCKS_PER_SLICE * USER_PAGES_PER_BLOCK * USER_CHANNELS * USER_WAYS))
    {
        endBlockNo++;
    }

    for(blockNo=startBlockNo ; blockNo<endBlockNo ; blockNo++)
        for(dieNo=0 ; dieNo<USER_DIES ; dieNo++)
        {
            reqSlotTag = GetFromFreeReqQ();

            reqPoolPtr->reqPool[reqSlotTag].reqType = REQ_TYPE_NAND;
            reqPoolPtr->reqPool[reqSlotTag].reqCode = REQ_CODE_ERASE;
            reqPoolPtr->reqPool[reqSlotTag].reqOpt.nandAddr = REQ_OPT_NAND_ADDR_PHY_ORG;
            reqPoolPtr->reqPool[reqSlotTag].reqOpt.dataBufFormat = REQ_OPT_DATA_BUF_NONE;
            reqPoolPtr->reqPool[reqSlotTag].reqOpt.forceUnitAccess = REQ_OPT_FORCE_UNIT_ACCESS_ON;
            reqPoolPtr->reqPool[reqSlotTag].reqOpt.blockSpace = REQ_OPT_BLOCK_SPACE_TOTAL;

            reqPoolPtr->reqPool[reqSlotTag].nandInfo.physicalCh = Vdie2PchTranslation(dieNo);
            reqPoolPtr->reqPool[reqSlotTag].nandInfo.physicalWay = Vdie2PwayTranslation(dieNo);
            reqPoolPtr->reqPool[reqSlotTag].nandInfo.physicalBlock = blockNo;
            reqPoolPtr->reqPool[reqSlotTag].nandInfo.physicalPage = 0;

			if(reqPoolPtr->reqPool[reqSlotTag].reqOpt.forceUnitAccess == REQ_OPT_FORCE_UNIT_ACCESS_ON)
				AllocateNotCompletedNandReqforNvmeIoCmd(reqPoolPtr->reqPool[reqSlotTag].nvmeCmdSlotTag);

            SelectLowLevelReqQ(reqSlotTag);
        }
}

void handle_nvme_io_read(unsigned int cmdSlotTag, NVME_IO_COMMAND *nvmeIOCmd)
{
	IO_READ_COMMAND_DW12 readInfo12;
	//IO_READ_COMMAND_DW13 readInfo13;
	//IO_READ_COMMAND_DW15 readInfo15;
	unsigned int startLba[2];
	unsigned int nlb;
	NVME_COMPLETION nvmeCPL;
    uint16_t status;

	readInfo12.dword = nvmeIOCmd->dword[12];
	//readInfo13.dword = nvmeIOCmd->dword[13];
	//readInfo15.dword = nvmeIOCmd->dword[15];

	startLba[0] = nvmeIOCmd->dword[10];
	startLba[1] = nvmeIOCmd->dword[11];
	nlb = readInfo12.NLB;

	ASSERT(startLba[0] < storageCapacity_L && (startLba[1] < STORAGE_CAPACITY_H || startLba[1] == 0));
	//ASSERT(nlb < MAX_NUM_OF_NLB);
	ASSERT((nvmeIOCmd->PRP1[0] & 0x3) == 0 && (nvmeIOCmd->PRP2[0] & 0x3) == 0); //error
	ASSERT(nvmeIOCmd->PRP1[1] < 0x10000 && nvmeIOCmd->PRP2[1] < 0x10000);

    status = nvme_read(startLba[0], nlb);
    if(status)
    {
    	nvmeCPL.statusFieldWord = 0;
    	nvmeCPL.statusField.SCT = SCT_GENERIC_COMMAND_STATUS;
    	nvmeCPL.statusField.SC = status;
        nvmeCPL.specific = 0x0;
        set_auto_nvme_cpl(cmdSlotTag, nvmeCPL.specific, nvmeCPL.statusFieldWord);
    }
    else
    {
    	ReqTransNvmeToSlice(cmdSlotTag, startLba[0], nlb, IO_NVM_READ);
    }
}


void handle_nvme_io_write(unsigned int cmdSlotTag, NVME_IO_COMMAND *nvmeIOCmd)
{
	IO_READ_COMMAND_DW12 writeInfo12;
	//IO_READ_COMMAND_DW13 writeInfo13;
	//IO_READ_COMMAND_DW15 writeInfo15;
	unsigned int startLba[2];
	unsigned int nlb;

	writeInfo12.dword = nvmeIOCmd->dword[12];
	//writeInfo13.dword = nvmeIOCmd->dword[13];
	//writeInfo15.dword = nvmeIOCmd->dword[15];

	//if(writeInfo12.FUA == 1)
	//	xil_printf("write FUA\r\n");

	startLba[0] = nvmeIOCmd->dword[10];
	startLba[1] = nvmeIOCmd->dword[11];
	nlb = writeInfo12.NLB;

	ASSERT(startLba[0] < storageCapacity_L && (startLba[1] < STORAGE_CAPACITY_H || startLba[1] == 0));
	//ASSERT(nlb < MAX_NUM_OF_NLB);
	ASSERT((nvmeIOCmd->PRP1[0] & 0xF) == 0 && (nvmeIOCmd->PRP2[0] & 0xF) == 0);
	ASSERT(nvmeIOCmd->PRP1[1] < 0x10000 && nvmeIOCmd->PRP2[1] < 0x10000);

	ReqTransNvmeToSlice(cmdSlotTag, startLba[0], nlb, IO_NVM_WRITE);
}

static void handle_nvme_io_zone_mgmt_send(unsigned int cmdSlotTag, NVME_IO_COMMAND *nvmeIOCmd)
{
    char send_cpl = 0;
    uint16_t status = nvme_zone_mgmt_send(cmdSlotTag, (void *)nvmeIOCmd, &send_cpl);
    NVME_COMPLETION nvmeCPL;

    if(send_cpl)
    {
    	nvmeCPL.statusFieldWord = 0;
    	nvmeCPL.statusField.SCT = SCT_GENERIC_COMMAND_STATUS;
    	nvmeCPL.statusField.SC = status;
    	set_auto_nvme_cpl(cmdSlotTag, 0, nvmeCPL.statusFieldWord);
    }
}

static void handle_nvme_io_zone_mgmt_recv(unsigned int cmdSlotTag, NVME_IO_COMMAND *nvmeIOCmd)
{
    unsigned int pZoneManagementData = ZONE_MANAGEMENT_RECV_ADDR;
    unsigned int data_size = 0;
    uint16_t status = nvme_zone_mgmt_recv(pZoneManagementData, (void *)nvmeIOCmd, &data_size);
	unsigned int prp[2];
    NVME_COMPLETION nvmeCPL;

    prp[0] = nvmeIOCmd->PRP1[0];
    prp[1] = nvmeIOCmd->PRP1[1];

//  xil_printf("prpLen = %X, prp[1] = %X, prp[0] = %X\r\n",prpLen, prp[1], prp[0]);
    set_direct_tx_dma(pZoneManagementData, prp[1], prp[0], data_size);
    check_direct_tx_dma_done();

    nvmeCPL.statusFieldWord = 0;
    nvmeCPL.statusField.SCT = SCT_GENERIC_COMMAND_STATUS;
    nvmeCPL.statusField.SC = status;

	set_auto_nvme_cpl(cmdSlotTag, 0, nvmeCPL.statusFieldWord);
}

void handle_nvme_io_cmd(NVME_COMMAND *nvmeCmd)
{
	NVME_IO_COMMAND *nvmeIOCmd;
	NVME_COMPLETION nvmeCPL;
	unsigned int opc;

	nvmeIOCmd = (NVME_IO_COMMAND*)nvmeCmd->cmdDword;

	opc = (unsigned int)nvmeIOCmd->OPC;

	switch(opc)
	{
		case IO_NVM_FLUSH:
		{
			PRINT("IO Flush Command\r\n");
			nvmeCPL.dword[0] = 0;
			nvmeCPL.specific = 0x0;
			set_auto_nvme_cpl(nvmeCmd->cmdSlotTag, nvmeCPL.specific, nvmeCPL.statusFieldWord);
			break;
		}
		case IO_NVM_WRITE:
		{
			PRINT("IO Write Command\r\n");
			handle_nvme_io_write(nvmeCmd->cmdSlotTag, nvmeIOCmd);
			break;
		}
		case IO_NVM_READ:
		{
			PRINT("IO Read Command\r\n");
			handle_nvme_io_read(nvmeCmd->cmdSlotTag, nvmeIOCmd);
			break;
		}
		case IO_NVM_ZONE_MGMT_SEND:
		{
			PRINT("IO Zone Management Send Command\r\n");
			handle_nvme_io_zone_mgmt_send(nvmeCmd->cmdSlotTag, nvmeIOCmd);
			break;
		}
		case IO_NVM_ZONE_MGMT_RECV:
		{
			PRINT("IO Zone Management Receive Command\r\n");
			handle_nvme_io_zone_mgmt_recv(nvmeCmd->cmdSlotTag, nvmeIOCmd);
			break;
		}
		case IO_NVM_ZONE_APPEND:
		{
			PRINT("IO Zone Append Command\r\n");
			handle_nvme_io_read(nvmeCmd->cmdSlotTag, nvmeIOCmd);
			break;
		}
		case IO_NVM_WRITE_ZEROS:
		{
			PRINT("IO Write Zeros Command\r\n");
			nvmeCPL.dword[0] = 0;
			nvmeCPL.specific = 0x0;
			set_auto_nvme_cpl(nvmeCmd->cmdSlotTag, nvmeCPL.specific, nvmeCPL.statusFieldWord);
			break;
		}

		default:
		{
			xil_printf("Not Support IO Command OPC: 0x%X\r\n", opc);
			ASSERT(0);
			break;
		}
	}

#if (__IO_CMD_DONE_MESSAGE_PRINT)
    xil_printf("OPC = 0x%X\r\n", nvmeIOCmd->OPC);
    xil_printf("PRP1[63:32] = 0x%X, PRP1[31:0] = 0x%X\r\n", nvmeIOCmd->PRP1[1], nvmeIOCmd->PRP1[0]);
    xil_printf("PRP2[63:32] = 0x%X, PRP2[31:0] = 0x%X\r\n", nvmeIOCmd->PRP2[1], nvmeIOCmd->PRP2[0]);
    xil_printf("dword10 = 0x%X\r\n", nvmeIOCmd->dword10);
    xil_printf("dword11 = 0x%X\r\n", nvmeIOCmd->dword11);
    xil_printf("dword12 = 0x%X\r\n", nvmeIOCmd->dword12);
#endif
}

