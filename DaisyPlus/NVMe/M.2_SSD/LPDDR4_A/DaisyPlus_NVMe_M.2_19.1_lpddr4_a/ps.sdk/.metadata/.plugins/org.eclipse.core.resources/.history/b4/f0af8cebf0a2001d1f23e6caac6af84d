//////////////////////////////////////////////////////////////////////////////////
// nvme_main.c for Cosmos+ OpenSSD
// Copyright (c) 2016 Hanyang University ENC Lab.
// Contributed by Yong Ho Song <yhsong@enc.hanyang.ac.kr>
//				  Youngjin Jo <yjjo@enc.hanyang.ac.kr>
//				  Sangjin Lee <sjlee@enc.hanyang.ac.kr>
//				  Jaewook Kwak <jwkwak@enc.hanyang.ac.kr>
//				  Kibin Park <kbpark@enc.hanyang.ac.kr>
//
// This file is part of Cosmos+ OpenSSD.
//
// Cosmos+ OpenSSD is free software; you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation; either version 3, or (at your option)
// any later version.
//
// Cosmos+ OpenSSD is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
// See the GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with Cosmos+ OpenSSD; see the file COPYING.
// If not, see <http://www.gnu.org/licenses/>.
//////////////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////////////
// Company: ENC Lab. <http://enc.hanyang.ac.kr>
// Engineer: Sangjin Lee <sjlee@enc.hanyang.ac.kr>
//			 Jaewook Kwak <jwkwak@enc.hanyang.ac.kr>
//			 Kibin Park <kbpark@enc.hanyang.ac.kr>
//
// Project Name: Cosmos+ OpenSSD
// Design Name: Cosmos+ Firmware
// Module Name: NVMe Main
// File Name: nvme_main.c
//
// Version: v1.2.0
//
// Description:
//   - initializes FTL and NAND
//   - handles NVMe controller
//////////////////////////////////////////////////////////////////////////////////

//////////////////////////////////////////////////////////////////////////////////
// Revision History:
//
// * v1.2.0
//   - header file for buffer is changed from "ia_lru_buffer.h" to "lru_buffer.h"
//   - Low level scheduler execution is allowed when there is no i/o command
//
// * v1.1.0
//   - DMA status initialization is added
//
// * v1.0.0
//   - First draft
//////////////////////////////////////////////////////////////////////////////////

#include "xil_printf.h"
#include "xstatus.h"
#include "debug.h"
#include "io_access.h"

#include "nvme.h"
#include "host_lld.h"
#include "nvme_main.h"
#include "nvme_admin_cmd.h"
#include "nvme_io_cmd.h"

#include "../memory_map.h"
#include "nvme_api.h"

volatile NVME_CONTEXT g_nvmeTask;
unsigned int nvme_storage[NUM_NVME];

void nvme_main()
{
	unsigned int rstCnt = 0;
	int Status;

	Status = init_nvme_rp(nvme_storage);
	if (Status != XST_SUCCESS) {
		xil_printf("init_nvme() Failed\r\n");
		return XST_FAILURE;
	}
	xil_printf("[ storage capacity %d MB ]\r\n", STORAGE_CAPACITY_L / ((1024*1024) / BYTES_PER_NVME_BLOCK));

	xil_printf("Turn on the host PC \r\n");

#if 1
    {
#define MAX_NLB 128

        unsigned int total_lba = nvme_storage[0], nlb = 0;
        unsigned int startLba = 0, seed = 0, i = 0, j = 0, devAddr = DATA_BUFFER_BASE_ADDR;
        unsigned long long *ptr;

        while(total_lba)
        {
            if(total_lba > MAX_NLB)
            {
                nlb = MAX_NLB;
            }
            else
            {
                nlb = total_lba;
            }

            ptr = (unsigned long long *)DATA_BUFFER_BASE_ADDR;

            for(i = 0; i < MAX_NLB * 4 * 1024 / sizeof(unsigned long long); i++)
            {
                *ptr++ = seed++;
            }

            write_nvme(0, startLba, devAddr, nlb);

            total_lba -= nlb;
            startLba += nlb;

            if(++j % 2048 == 0)
            {
                xil_printf("%GB\r\n", j / 2048);
            }
        }

        total_lba = nvme_storage[0];
        nlb = 0;
        startLba = 0;
        seed = 0
        while(total_lba)
        {
            if(total_lba > MAX_NLB)
            {
                nlb = MAX_NLB;
            }
            else
            {
                nlb = total_lba;
            }

            ptr = (unsigned long long *)DATA_BUFFER_BASE_ADDR;

            read_nvme(0, startLba, devAddr, nlb);

            for(i = 0; i < nlb * 4 * 1024 / sizeof(unsigned long long); i++)
            {
                if(*ptr != seed)
                {
                    xil_printf("*ptr=%x, seed=%x\r\n", *ptr, seed);
                }
                ptr++;
                seed++;
            }

            total_lba -= nlb;
            startLba += nlb;

            if(++j % 2048 == 0)
            {
                xil_printf("%GB\r\n", j / 2048);
            }
        }

        total_lba = nvme_storage[1];
        nlb = 0;
        startLba = 0;
        seed = 0

        while(total_lba)
        {
            if(total_lba > MAX_NLB)
            {
                nlb = MAX_NLB;
            }
            else
            {
                nlb = total_lba;
            }

            ptr = (unsigned long long *)DATA_BUFFER_BASE_ADDR;

            for(i = 0; i < MAX_NLB * 4 * 1024 / sizeof(unsigned long long); i++)
            {
                *ptr++ = seed++;
            }

            write_nvme(1, startLba, devAddr, nlb);

            total_lba -= nlb;
            startLba += nlb;

            if(++j % 2048 == 0)
            {
                xil_printf("%GB\r\n", j / 2048);
            }
        }

        total_lba = nvme_storage[1];
        nlb = 0;
        startLba = 0;
        seed = 0
        while(total_lba)
        {
            if(total_lba > MAX_NLB)
            {
                nlb = MAX_NLB;
            }
            else
            {
                nlb = total_lba;
            }

            ptr = (unsigned long long *)DATA_BUFFER_BASE_ADDR;

            read_nvme(1, startLba, devAddr, nlb);

            for(i = 0; i < nlb * 4 * 1024 / sizeof(unsigned long long); i++)
            {
                if(*ptr != seed)
                {
                    xil_printf("*ptr=%x, seed=%x\r\n", *ptr, seed);
                }
                ptr++;
                seed++;
            }

            total_lba -= nlb;
            startLba += nlb;

            if(++j % 2048 == 0)
            {
                xil_printf("%GB\r\n", j / 2048);
            }
        }
#if 0
        int devAddr = DATA_BUFFER_BASE_ADDR;
        int *ptr = (int *)DATA_BUFFER_BASE_ADDR;
        int i;

        for(i = 0; i < 512*1024 / sizeof(int); i++)
        {
            *ptr++ = i;
        }

        ptr = (int *)(DATA_BUFFER_BASE_ADDR + 1024*1024);
        for(i = 0; i < 512*1024 / sizeof(int); i++)
        {
            *ptr++ = i;
        }

        ptr = (int *)(DATA_BUFFER_BASE_ADDR + 2*1024*1024);
        for(i = 0; i < 512*1024 / sizeof(int); i++)
        {
            *ptr++ = i + 0xFFFF0000;
        }

        write_nvme(0, 0, devAddr, 16);
        devAddr = (DATA_BUFFER_BASE_ADDR + 2*1024*1024);
        write_nvme(0, 1, devAddr, 9);

        devAddr = DATA_BUFFER_BASE_ADDR;
        read_nvme(0, 0, devAddr, 16);

        devAddr = DATA_BUFFER_BASE_ADDR + 1024*1024;
        write_nvme(1, 0, devAddr, 16);
        devAddr = (DATA_BUFFER_BASE_ADDR + 2*1024*1024);
        write_nvme(1, 1, devAddr, 9);

        devAddr = DATA_BUFFER_BASE_ADDR + 1024*1024;
        read_nvme(1, 0, devAddr, 16);
#endif
    }
#endif

	while(1)
	{
		if(g_nvmeTask.status == NVME_TASK_WAIT_CC_EN)
		{
			unsigned int ccEn;
			ccEn = check_nvme_cc_en();
			if(ccEn == 1)
			{
				set_nvme_admin_queue(1, 1, 1);
				set_nvme_csts_rdy(1);
				g_nvmeTask.status = NVME_TASK_RUNNING;
				xil_printf("\r\nNVMe ready!!!\r\n");
			}
		}
		else if(g_nvmeTask.status == NVME_TASK_RUNNING)
		{
			NVME_COMMAND nvmeCmd;
			unsigned int cmdValid;

			cmdValid = get_nvme_cmd(&nvmeCmd.qID, &nvmeCmd.cmdSlotTag, &nvmeCmd.cmdSeqNum, nvmeCmd.cmdDword);

			if(cmdValid == 1)
			{
				rstCnt = 0;
				if(nvmeCmd.qID == 0)
				{
					handle_nvme_admin_cmd(&nvmeCmd);
				}
				else
				{
					handle_nvme_io_cmd(&nvmeCmd);
				}
			}
		}
		else if(g_nvmeTask.status == NVME_TASK_SHUTDOWN)
		{
			NVME_STATUS_REG nvmeReg;
			nvmeReg.dword = IO_READ32(NVME_STATUS_REG_ADDR);
			if(nvmeReg.ccShn != 0)
			{
				unsigned int qID;
				set_nvme_csts_shst(1);

				for(qID = 0; qID < 8; qID++)
				{
					set_io_cq(qID, 0, 0, 0, 0, 0, 0);
					set_io_sq(qID, 0, 0, 0, 0, 0);
				}

				set_nvme_admin_queue(0, 0, 0);
				g_nvmeTask.cacheEn = 0;
				set_nvme_csts_shst(2);
				g_nvmeTask.status = NVME_TASK_WAIT_RESET;

				xil_printf("\r\nNVMe shutdown!!!\r\n");
			}
		}
		else if(g_nvmeTask.status == NVME_TASK_WAIT_RESET)
		{
			unsigned int ccEn;
			ccEn = check_nvme_cc_en();
			if(ccEn == 0)
			{
                unsigned int qID;

				g_nvmeTask.cacheEn = 0;
				set_nvme_csts_shst(0);
				set_nvme_csts_rdy(0);

                set_nvme_admin_queue(0, 0, 0);
                for(qID = 0; qID < 8; qID++)
                {
                    set_io_cq(qID, 0, 0, 0, 0, 0, 0);
                    set_io_sq(qID, 0, 0, 0, 0, 0);
                }

				g_nvmeTask.status = NVME_TASK_IDLE;
				xil_printf("\r\nNVMe disable!!!\r\n");
			}
		}
		else if(g_nvmeTask.status == NVME_TASK_RESET)
		{
			unsigned int qID;
			for(qID = 0; qID < 8; qID++)
			{
				set_io_cq(qID, 0, 0, 0, 0, 0, 0);
				set_io_sq(qID, 0, 0, 0, 0, 0);
			}

			if (rstCnt== 5){
				pcie_async_reset(rstCnt);
				rstCnt = 0;
				xil_printf("\r\nPcie iink disable!!!\r\n");
				xil_printf("Wait few minute or reconnect the PCIe cable\r\n");
			}
			else
				rstCnt++;

			g_nvmeTask.cacheEn = 0;
			set_nvme_admin_queue(0, 0, 0);
			set_nvme_csts_shst(0);
			set_nvme_csts_rdy(0);
			g_nvmeTask.status = NVME_TASK_IDLE;

			xil_printf("\r\nNVMe reset!!!\r\n");
		}
	}
}


